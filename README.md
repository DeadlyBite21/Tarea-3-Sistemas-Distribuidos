# üê∑ Tarea 3: An√°lisis Ling√º√≠stico Offline con Hadoop y Pig

Este repositorio contiene la soluci√≥n para la **Tarea 3** del curso de **Sistemas Distribuidos**. El proyecto implementa un sistema de procesamiento batch utilizando **Apache Hadoop (HDFS)** y **Apache Pig** para realizar un an√°lisis comparativo de vocabulario entre respuestas humanas y respuestas generadas por Grandes Modelos de Lenguaje (LLMs).

üöÄ El sistema est√° completamente contenerizado con **Docker** para facilitar su despliegue y ejecuci√≥n en cualquier entorno.

---

## üìã Requisitos Previos

Para ejecutar este proyecto necesitas tener instalado:

- **Docker** y **Docker Compose**.
- **Python 3.x** (para el pre-procesamiento de datos).
- Archivo `dataset.csv` original (debe colocarse en la carpeta `data/`).

---

## üìÇ Estructura del Proyecto

El proyecto mantiene la siguiente estructura de archivos y carpetas:

```text
.
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestaci√≥n de contenedores (Namenode, Datanode, Pig Client)
‚îú‚îÄ‚îÄ hadoop.env              # Variables de entorno para configuraci√≥n del cl√∫ster Hadoop
‚îú‚îÄ‚îÄ filtrar_datos.py        # Script Python para limpieza y reducci√≥n del dataset inicial
‚îú‚îÄ‚îÄ README.md               # Este archivo
‚îú‚îÄ‚îÄ data/                   # Carpeta local para datos de entrada y salida
‚îÇ   ‚îú‚îÄ‚îÄ dataset.csv         # Archivo original
‚îÇ   ‚îî‚îÄ‚îÄ stopwords.txt       # Lista de palabras vac√≠as a ignorar
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ analisis.pig        # Script de Pig Latin con la l√≥gica MapReduce
‚îî‚îÄ‚îÄ pig-image/
    ‚îî‚îÄ‚îÄ Dockerfile          # Imagen personalizada que instala Pig y Piggybank.jar
```

---

## üõ†Ô∏è Despliegue del Cl√∫ster Hadoop

Construye las im√°genes y levanta los contenedores. La imagen personalizada de Pig descargar√° autom√°ticamente las librer√≠as necesarias (`piggybank.jar`).

```bash
docker-compose up -d --build
```

> [!IMPORTANT]
> Espera unos minutos (1-2 min) hasta que los servicios de Hadoop arranquen por completo.

Puedes verificar que el nodo de datos est√° activo con:

```bash
docker exec -it namenode hdfs dfsadmin -report
```

---

## üì• Ingesta de Datos a HDFS

Una vez el cl√∫ster est√° activo, subimos el dataset filtrado y la lista de stopwords al sistema de archivos distribuido (HDFS).

### 1. Crear directorio de entrada en HDFS
```bash
docker exec -it namenode hdfs dfs -mkdir -p /user/input
```

### 2. Subir el dataset filtrado
```bash
docker exec -it namenode hdfs dfs -put /user/input/dataset_filtrado.csv /user/input/dataset_filtrado.csv
```

### 3. Subir las stopwords
> [!NOTE]
> Esto es indispensable para el script Pig.

```bash
docker exec -it namenode hdfs dfs -put /user/input/stopwords.txt /user/input/stopwords_en.txt
```

---

## ‚öôÔ∏è Ejecuci√≥n del An√°lisis (MapReduce)

Ejecutamos el script de Pig Latin dentro del contenedor cliente. Este script realiza:

1.  **Tokenizaci√≥n** de textos.
2.  **Limpieza y normalizaci√≥n**.
3.  **Filtrado de stopwords**.
4.  **Conteo de frecuencia** (WordCount).
5.  **C√°lculo del Top 50** palabras m√°s usadas por Humanos y por la IA.

```bash
docker exec -it pig-client pig -x mapreduce /scripts/analisis.pig
```

---

## üìä Obtenci√≥n de Resultados

Cuando el proceso termine con el mensaje `Success!`, descarga los resultados desde HDFS a tu carpeta local `data/` para analizarlos.

### Descargar Top 50 palabras de Humanos
```bash
docker exec -it namenode hdfs dfs -getmerge /user/output/human_top_50 /user/input/human_top_50.csv
```

### Descargar Top 50 palabras de LLM
```bash
docker exec -it namenode hdfs dfs -getmerge /user/output/llm_top_50 /user/input/llm_top_50.csv
```
